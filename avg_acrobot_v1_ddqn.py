# -*- coding: utf-8 -*-
"""Avg_Acrobot_v1_DDQN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/170sK_ss-dZfMdBtYuEZYFCCCf3Rkgagb
"""

import gymnasium as gym
import torch
import torch.nn as nn
import numpy as np
from collections import deque
import random
from itertools import count
import torch.nn.functional as F
from tqdm import tqdm
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
import wandb

wandb.login(key="4ed258ed47e8130e3c5cd1be33b9be8abf3b585c")


class QNetwork(nn.Module):
    def __init__(self,fc1_unit,fc2_unit,adv_type):
        super(QNetwork, self).__init__()
        self.fc1 = nn.Linear(6, fc1_unit)  # 6 input features for Acrobot
        self.relu = nn.ReLU()
        self.fc_value = nn.Linear(fc1_unit, fc2_unit)
        self.fc_adv = nn.Linear(fc1_unit, fc2_unit)
        self.value = nn.Linear(fc2_unit, 1)
        self.adv = nn.Linear(fc2_unit, 3)  # 3 possible actions in Acrobot
        self.adv_type = adv_type


    def forward(self, state):
        y = self.relu(self.fc1(state))
        value = self.relu(self.fc_value(y))
        adv = self.relu(self.fc_adv(y))

        value = self.value(value)
        adv = self.adv(adv)

        # advAverage = torch.mean(adv, dim=1, keepdim=True)
        # Q = value + adv - advAverage
        if self.adv_type == 'avg':
            advAverage = torch.mean(adv, dim=1, keepdim=True)
            Q = value + adv - advAverage
        else:
            advMax, _ = torch.max(adv, dim=1, keepdim=True)
            Q = value + adv - advMax

        return Q

    def select_action(self, state):
        with torch.no_grad():
            Q = self.forward(state)
            action_index = torch.argmax(Q, dim=1)
        return action_index.item()


class Memory(object):
    def __init__(self, memory_size: int) -> None:
        self.memory_size = memory_size
        self.buffer = deque(maxlen=self.memory_size)

    def add(self, experience) -> None:
        self.buffer.append(experience)

    def size(self):
        return len(self.buffer)

    def sample(self, batch_size: int, continuous: bool = True):
        if batch_size > len(self.buffer):
            batch_size = len(self.buffer)
        if continuous:
            rand = random.randint(0, len(self.buffer) - batch_size)
            return [self.buffer[i] for i in range(rand, rand + batch_size)]
        else:
            indexes = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)
            return [self.buffer[i] for i in indexes]

    def clear(self):
        self.buffer.clear()

def train_ddqn(fc1_unit,fc2_unit,buffer_size,batch_size,update_steps,adv_type):
    seeds=[0,1,2,3,4]
    all_seed_rewards=[]
    all_seed_steps=[]
    all_seed_regrets=[]

    # network=QNetwork()
    # network.fc1=64
    for seed in seeds:
        print(seed)
        env = gym.make('Acrobot-v1')
        n_state = env.observation_space.shape[0]  # should be 6
        n_action = env.action_space.n  # should be 3

        onlineQNetwork = QNetwork(fc1_unit,fc2_unit,adv_type).to(device)
        targetQNetwork = QNetwork(fc1_unit,fc2_unit,adv_type).to(device)
        targetQNetwork.load_state_dict(onlineQNetwork.state_dict())

        optimizer = torch.optim.Adam(onlineQNetwork.parameters(), lr=1e-4)

        gamma = 0.99
        EXPLORE = 20000
        initil_eps = 1
        final_eps = 0.0001
        replay_memory = buffer_size
        batch_size = batch_size
        update_steps = update_steps

        memory_replay = Memory(replay_memory)

        epsilon = initil_eps
        learn_steps = 0
        begin_learn = False
        num_episodes=500
        total_rewards = np.zeros(num_episodes)
        total_regrets = np.zeros(num_episodes)
        total_steps = np.zeros(num_episodes)
        for ep in range(num_episodes):
            state, _ = env.reset()
            ep_reward = 0
            steps=0
            done = False

            # for time_steps in range(500):
            while not done:

                p = random.random()
                if p < epsilon:
                    action = random.randint(0, n_action - 1)
                else:
                    tensor_state = torch.FloatTensor(state).unsqueeze(0).to(device)
                    action = onlineQNetwork.select_action(tensor_state)

                next_state, reward, terminated, truncated, _ = env.step(action)
                done = terminated or truncated
                ep_reward += reward

                memory_replay.add((state, next_state, action, reward, done))
                steps+=1
                if memory_replay.size() > 128:
                    if not begin_learn:
                        # print('learn begin!')
                        begin_learn = True

                    learn_steps += 1
                    if learn_steps % update_steps == 0:
                        targetQNetwork.load_state_dict(onlineQNetwork.state_dict())

                    batch = memory_replay.sample(batch_size, False)
                    batch_state, batch_next_state, batch_action, batch_reward, batch_done = zip(*batch)
                    batch_state = np.array(batch_state)
                    batch_state = torch.FloatTensor(batch_state).to(device)

                    batch_next_state = np.array(batch_next_state)
                    batch_next_state = torch.FloatTensor(batch_next_state).to(device)
                    batch_action = torch.LongTensor(batch_action).unsqueeze(1).to(device)
                    batch_reward = torch.FloatTensor(batch_reward).unsqueeze(1).to(device)
                    batch_done = torch.FloatTensor(batch_done).unsqueeze(1).to(device)

                    with torch.no_grad():
                        onlineQ_next = onlineQNetwork(batch_next_state)
                        targetQ_next = targetQNetwork(batch_next_state)
                        online_max_action = torch.argmax(onlineQ_next, dim=1, keepdim=True)
                        y = batch_reward + (1 - batch_done) * gamma * targetQ_next.gather(1, online_max_action)

                    current_Q = onlineQNetwork(batch_state).gather(1, batch_action)
                    loss = F.mse_loss(current_Q, y)

                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()

                    if epsilon > final_eps:
                        epsilon -= (initil_eps - final_eps) / EXPLORE
                regret=steps

                if done:
                    # if terminated:
                    #     print("Envvironment Solved",steps)
                    if truncated:
                        regret+=100
                    break
                state = next_state
            total_regrets[ep]=steps
            total_rewards[ep]=ep_reward
            # total_modified_rewards[ep]=ep_mod_reward
            total_steps[ep]=steps
            env.close()
            # if epoch % 10 == 0:
            #     torch.save(onlineQNetwork.state_dict(), 'ddqn-acrobot-policy.para')
            #     print(f'Episode {epoch} | Reward: {episode_reward:.2f}')
        all_seed_rewards.append(total_rewards)
        all_seed_steps.append(total_steps)
        all_seed_regrets.append(total_regrets)
    all_seed_rewards_mean=np.mean(all_seed_rewards,axis=0)
    all_seed_steps_mean=np.mean(all_seed_steps,axis=0)
    all_seed_regrets_mean=np.mean(all_seed_regrets,axis=0)
    for ep in range(num_episodes):
            wandb.log({
                "Episode": ep,
                "Mean Episodic Return": all_seed_rewards_mean[ep],
                "Mean Steps": all_seed_steps_mean[ep],
                "Mean Regret": all_seed_regrets_mean[ep],
                "fc1": fc1_unit,
                "fc2": fc2_unit,
                "buffer_size": buffer_size,
                "update_steps":update_steps,
                "adv_type":adv_type,
                "batch_size":batch_size,
            })
    return all_seed_rewards_mean,all_seed_steps_mean,all_seed_regrets_mean

sweep_config = {
    "method": "bayes",  # Bayesian optimization
    "metric": {"name": "Mean Regret", "goal": "minimize"},  # Minimize regret
    "parameters": {
        "fc1_unit": {"values": [64,128,256]},
        "fc2_unit": {"values": [128,256]},
        "buffer_size": {"values": [10000,25000,50000]},
        "batch_size":{"values":[16,64,128]},
        "update_steps":{"values":[4,8]},
        "adv_type":{"values":["avg"]},

    },
}

def sweep_train():

    wandb.init(project="avg_dueling_dqn_acrobot_v1_run_0", name=" Avg Dueling DQN  Hyper parameter tuninh",config=sweep_config)
    config = wandb.config
    fc1_unit=config.fc1_unit
    fc2_unit=config.fc2_unit
    buffer_size=config.buffer_size
    batch_size=config.batch_size
    update_steps=config.update_steps
    adv_type=config.adv_type
    # current_temp = config.temp_start
    wandb.run.name = "fc1_{}_fc2_{}_bfs_{}_bs_{}_ups_{}_{}".format(fc1_unit,fc2_unit,buffer_size,batch_size,update_steps,adv_type)

    all_seed_rewards_mean,all_seed_steps_mean,all_seed_regrets_mean= train_ddqn(fc1_unit,fc2_unit,buffer_size,batch_size,update_steps,adv_type)

sweep_id = wandb.sweep(sweep_config, project="avg_dueling_dqn_acrobot_v1_run_0")
wandb.agent(sweep_id, function=sweep_train,project="avg_dueling_dqn_acrobot_v1_run_0", count=30)





